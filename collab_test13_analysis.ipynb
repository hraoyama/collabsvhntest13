{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "collab_test13_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "av22_gwc_hOd",
        "-7HAq0FaDTAA",
        "7iyyimVKOpEl"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av22_gwc_hOd"
      },
      "source": [
        "# Setup and install for test13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YMtSG2yb-pD"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l247Lv_MYR8O",
        "outputId": "d0f92df5-8f56-4314-b4f2-d3130625242a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqquQY0B65AH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e82e815-1870-47df-dd36-063e8b6a1e73"
      },
      "source": [
        "%pip install gpflow\n",
        "%pip install plotnine\n",
        "%pip install plotly\n",
        "%pip install keras_self_attention\n",
        "%pip install keras_tuner\n",
        "%pip install pycm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gpflow in /usr/local/lib/python3.7/dist-packages (2.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gpflow) (1.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gpflow) (21.3)\n",
            "Requirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from gpflow) (2.7.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from gpflow) (3.10.0.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from gpflow) (0.8.9)\n",
            "Requirement already satisfied: deprecated in /usr/local/lib/python3.7/dist-packages (from gpflow) (1.2.13)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from gpflow) (0.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from gpflow) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-probability>0.10.0 in /usr/local/lib/python3.7/dist-packages (from gpflow) (0.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gpflow) (1.19.5)\n",
            "Requirement already satisfied: multipledispatch>=0.6 in /usr/local/lib/python3.7/dist-packages (from gpflow) (0.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from multipledispatch>=0.6->gpflow) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->gpflow) (1.13.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->gpflow) (3.17.3)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->gpflow) (2.7.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->gpflow) (12.0.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->gpflow) (1.6.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->gpflow) (0.37.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->gpflow) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->gpflow) (0.22.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->gpflow) (2.7.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->gpflow) (2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->gpflow) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->gpflow) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->gpflow) (2.7.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->gpflow) (0.12.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->gpflow) (3.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->gpflow) (1.42.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->gpflow) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->gpflow) (0.2.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.2.0->gpflow) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2.0->gpflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2.0->gpflow) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2.0->gpflow) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2.0->gpflow) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2.0->gpflow) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2.0->gpflow) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.2.0->gpflow) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.2.0->gpflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.2.0->gpflow) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.2.0->gpflow) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.2.0->gpflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.2.0->gpflow) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.2.0->gpflow) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.2.0->gpflow) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.2.0->gpflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.2.0->gpflow) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.2.0->gpflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow>=2.2.0->gpflow) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.2.0->gpflow) (3.1.1)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>0.10.0->gpflow) (1.3.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>0.10.0->gpflow) (4.4.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>0.10.0->gpflow) (0.1.6)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gpflow) (3.0.6)\n",
            "Requirement already satisfied: plotnine in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: descartes>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.19.5)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from plotnine) (0.5.2)\n",
            "Requirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (1.1.5)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (0.10.2)\n",
            "Requirement already satisfied: mizani>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from plotnine) (0.6.0)\n",
            "Requirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.7/dist-packages (from plotnine) (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (3.0.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->plotnine) (1.3.2)\n",
            "Requirement already satisfied: palettable in /usr/local/lib/python3.7/dist-packages (from mizani>=0.6.0->plotnine) (3.3.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->plotnine) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.4.1->plotnine) (1.15.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (4.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly) (1.15.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly) (1.3.3)\n",
            "Requirement already satisfied: keras_self_attention in /usr/local/lib/python3.7/dist-packages (0.50.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras_self_attention) (1.19.5)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras_self_attention) (2.7.0)\n",
            "Requirement already satisfied: keras_tuner in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (21.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (1.19.5)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (2.7.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (1.4.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (5.5.0)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (1.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (2.23.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (5.1.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (57.4.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras_tuner) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras_tuner) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras_tuner) (3.0.6)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras_tuner) (0.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras_tuner) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras_tuner) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras_tuner) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras_tuner) (2.10)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (1.35.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (0.12.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (0.4.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (0.37.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (1.8.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (1.42.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (3.3.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras_tuner) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras_tuner) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras_tuner) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras_tuner) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras_tuner) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras_tuner) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras_tuner) (3.10.0.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras_tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras_tuner) (3.1.1)\n",
            "Requirement already satisfied: pycm in /usr/local/lib/python3.7/dist-packages (3.3)\n",
            "Requirement already satisfied: art>=1.8 in /usr/local/lib/python3.7/dist-packages (from pycm) (5.3)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from pycm) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlJWalkeWNcw"
      },
      "source": [
        "from plotnine import *\n",
        "from plotnine.themes import *\n",
        "import plotly as py\n",
        "import plotly.graph_objs as go\n",
        "import pandas as pd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmUjYbArAuQT"
      },
      "source": [
        "import tensorflow as tf\n",
        "from scipy.io import loadmat\n",
        "import random\n",
        "import math\n",
        "import tensorflow_probability as tfp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PieVKPfHHYQ6"
      },
      "source": [
        "_paper_name_ establishes the reusable name of the paper, it represents the directory under data_papers on the google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BI4p7ZKb0Qz2"
      },
      "source": [
        "paper_name = \"test13\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "433z6V3T2rB2"
      },
      "source": [
        "import os, sys\n",
        "import errno\n",
        "\n",
        "# make a directory if it does not exist\n",
        "def make_dir_if_not_exist(used_path):\n",
        "    if not os.path.isdir(used_path):\n",
        "        try:\n",
        "            os.mkdir(used_path)\n",
        "        except OSError as exc:\n",
        "            if exc.errno != errno.EEXIST:\n",
        "                raise exc\n",
        "            else:\n",
        "                raise ValueError(f'{used_path} directoy cannot be created because its parent directory does not exist.')\n",
        "\n",
        "# make directories if they do not exist\n",
        "\n",
        "make_dir_if_not_exist(\"/content/drive/MyDrive/data_papers/\")\n",
        "make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}\")\n",
        "make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features/\")\n",
        "make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/\")\n",
        "make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_history/\")\n",
        "make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/\")\n",
        "make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/gp_collab/\")\n",
        "make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_predictions/\")\n",
        "make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_ccs/\")\n",
        "make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/\")\n",
        "make_dir_if_not_exist(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/temp/\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yCKQpti4HWJ"
      },
      "source": [
        "data_location = f'/content/drive/MyDrive/data_papers/{paper_name}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fg0OYkHWkvbj"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import  Dense, Flatten, Activation, Dropout, Embedding, Conv1D, MaxPooling1D, GRU\n",
        "from tensorflow.keras.layers import LSTM, TimeDistributed, Permute,Reshape, Lambda, RepeatVector, Input,Multiply\n",
        "from tensorflow.keras.layers import  Dense, Flatten, Activation, Dropout, Embedding, Conv1D, Conv2D, MaxPooling2D, MaxPooling1D, Concatenate, BatchNormalization, GaussianNoise\n",
        "from tensorflow.keras.layers import SimpleRNN, GRU, LeakyReLU\n",
        "from tensorflow.keras.layers import Concatenate, Average \n",
        "from tensorflow.keras.models import Sequential, load_model, Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import  Bidirectional\n",
        "from timeit import default_timer as timer\n",
        "import h5py as h5\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Model\n",
        "import errno\n",
        "from tensorflow.keras.applications.imagenet_utils import decode_predictions\n",
        "import random\n",
        "import warnings\n",
        "import gpflow\n",
        "from gpflow.utilities import ops, print_summary, set_trainable\n",
        "from gpflow.config import set_default_float, default_float, set_default_summary_fmt\n",
        "from gpflow.ci_utils import ci_niter\n",
        "import warnings\n",
        "from functools import partial\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from tensorflow.keras.layers import  Dense, Flatten, Activation, Dropout, Embedding, Conv1D, Conv2D, MaxPooling2D, MaxPooling1D, Concatenate, BatchNormalization, GaussianNoise\n",
        "from tensorflow.keras.layers import LSTM, TimeDistributed, Permute, Reshape, Lambda, RepeatVector, Input, Multiply, SimpleRNN, GRU, LeakyReLU\n",
        "from keras_self_attention import SeqSelfAttention, SeqWeightedAttention\n",
        "from tensorflow.keras import regularizers\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "import keras_tuner as kt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7--mKqbEhZZ"
      },
      "source": [
        "# Run this cell to load the dataset\n",
        "INPUT_DIM = 8    # \n",
        "\n",
        "hf_Train = h5.File(f'{data_location}/Fold_10_Train_Data_1000.h5', 'r')\n",
        "hf_Test = h5.File(f'{data_location}/Fold_10_Test_Data_1000.h5', 'r')\n",
        "\n",
        "X_train = hf_Train['Train_Data'] # Get train set\n",
        "X_train = np.array(X_train)\n",
        "Y_train = hf_Train['Label']      # Get train label\n",
        "Y_train = np.array(Y_train)\n",
        "\n",
        "X_test = hf_Test['Train_Data']     # Get test set\n",
        "X_test = np.array(X_test)\n",
        "Y_test = hf_Test['Label']       # Get test label\n",
        "Y_test = np.array(Y_test)\n",
        "\n",
        "Y_train = to_categorical(Y_train, 13)  # Process the label of tain\n",
        "Y_test = to_categorical(Y_test, 13)    #  Process the label of te\n",
        "\n",
        "def coShuffled_vectors(X,Y):\n",
        "    if tf.shape(X)[0] ==  tf.shape(Y)[0]:\n",
        "        test_idxs = tf.range(start=0, limit=tf.shape(X)[0], dtype=tf.int32)\n",
        "        shuffled_test_idxs = tf.random.shuffle(test_idxs)\n",
        "        return ( tf.gather(X, shuffled_test_idxs), tf.gather(Y, shuffled_test_idxs) )\n",
        "    else:\n",
        "        raise ValueError(f\"0-dimension has to be the same {tf.shape(X)[0]} != {tf.shape(Y)[0]}\")\n",
        "\n",
        "X_test_shuffled, Y_test_shuffled = coShuffled_vectors(np.array(hf_Test['Train_Data']), to_categorical(np.array(hf_Test['Label']),13))\n",
        "X_train_shuffled, Y_train_shuffled = coShuffled_vectors(np.array(hf_Train['Train_Data']), to_categorical(np.array(hf_Train['Label']),13))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b7tNfakowYM"
      },
      "source": [
        "def plot_history(history):\n",
        "    acc_keys = [k for k in history.history.keys() if k in ('accuracy', 'val_accuracy')] \n",
        "    loss_keys = [k for k in history.history.keys() if not k in acc_keys]\n",
        "    for k, v in history.history.items():\n",
        "        if k in acc_keys:\n",
        "            plt.figure(1)\n",
        "            plt.plot(v)\n",
        "        else:\n",
        "            plt.figure(2)\n",
        "            plt.plot(v)\n",
        "    plt.figure(1)\n",
        "    plt.title('Accuracy vs. epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(acc_keys, loc='upper right')\n",
        "    plt.figure(2)\n",
        "    plt.title('Loss vs. epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(loss_keys, loc='upper right')\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5hVSGiwo3t_"
      },
      "source": [
        "\n",
        "def get_layer_by_name(layers, name, return_first = True):\n",
        "    matching_named_layers = [ l for l in layers if l.name==name]\n",
        "    if not matching_named_layers:\n",
        "        return None\n",
        "    return matching_named_layers[0] if return_first else matching_named_layers\n",
        "\n",
        "\n",
        "def get_combined_features_from_models(\n",
        "        to_combine, \n",
        "        X_train, Y_train, \n",
        "        X_test, Y_test,\n",
        "        reverse_one_hot = False,\n",
        "        normalize_X_func = None):\n",
        "    \n",
        "    models = dict()\n",
        "    X_trains_out = []\n",
        "    X_test_out = []\n",
        "    XY_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: None))))\n",
        "    \n",
        "    if reverse_one_hot:\n",
        "        Y_train_new = np.apply_along_axis(np.argmax, 1, Y_train) + 1  \n",
        "        Y_test_new = np.apply_along_axis(np.argmax, 1, Y_test) + 1 \n",
        "    else:\n",
        "        Y_train_new = Y_train.copy()\n",
        "        Y_test_new = Y_test.copy()\n",
        "    \n",
        "    for model_file_name, layer_name, kwargs in to_combine:\n",
        "        model_here = None\n",
        "        if isinstance(model_file_name, tf.keras.models.Model):\n",
        "            model_here = model_file_name\n",
        "            model_file_name = model_here.name\n",
        "        else:\n",
        "            if model_file_name in models.keys():\n",
        "                model_here = models[model_file_name]\n",
        "            else:\n",
        "                model_here = tf.keras.models.load_model(model_file_name, **kwargs) if kwargs is not None else tf.keras.models.load_model(model_file_name)\n",
        "        features_model = Model(model_here.input, \n",
        "                               get_layer_by_name(model_here.layers,layer_name).output)\n",
        "        if normalize_X_func is None:\n",
        "            X_trains_out.append(np.array(features_model.predict(X_train), dtype='float64'))\n",
        "            X_test_out.append(np.array(features_model.predict(X_test), dtype='float64'))\n",
        "        else:\n",
        "            X_trains_out.append(np.array(normalize_X_func(features_model.predict(X_train)), dtype='float64'))\n",
        "            X_test_out.append(np.array(normalize_X_func(features_model.predict(X_test)), dtype='float64'))\n",
        "        XY_dict[model_file_name][layer_name]['Train']['X'] = X_trains_out[-1]\n",
        "        XY_dict[model_file_name][layer_name]['Test']['X'] = X_test_out[-1]\n",
        "        XY_dict[model_file_name][layer_name]['Train']['Y'] = Y_train_new\n",
        "        XY_dict[model_file_name][layer_name]['Test']['Y'] = Y_test_new\n",
        "        models[model_file_name] = model_here            \n",
        "        \n",
        "    X_train_new = np.concatenate(tuple(X_trains_out), axis = 1)\n",
        "    X_test_new = np.concatenate(tuple(X_test_out), axis = 1)\n",
        "    \n",
        "    data_train = (X_train_new, Y_train_new)\n",
        "    data_test = (X_test_new, Y_test_new)    \n",
        "    \n",
        "        \n",
        "    return (models, data_train, data_test,XY_dict)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyVEfMk4pDEa"
      },
      "source": [
        "def run_and_save_model(model_func, X_train, Y_train, kwargs):    \n",
        "    m = model_func()\n",
        "    m.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
        "    history = m.fit(X_train, Y_train, **kwargs) \n",
        "    m.save(f\"{m.name}_Tenth_Fold_New_Model_500_8\") #Save the model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pycm import ConfusionMatrix"
      ],
      "metadata": {
        "id": "-4nBo4K9raov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxNjkxgjxc5y"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import h5py as h5\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv1D, LeakyReLU, MaxPooling1D, BatchNormalization, GaussianNoise, Dropout, Dense, Flatten\n",
        "import errno\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from tensorflow.summary import create_file_writer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import kerastuner as kt\n",
        "from kerastuner import HyperModel\n",
        "import numpy as np\n",
        "import itertools\n",
        "import multiprocessing\n",
        "from numpy import genfromtxt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from pycm import ConfusionMatrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aWfKrF40X4gX",
        "outputId": "08986284-7123-4ca5-d653-e5227bb45b78"
      },
      "source": [
        "data_location"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/data_papers/test13'"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87ma654Ixu3f"
      },
      "source": [
        "\n",
        "def coShuffled_vectors(X, Y):\n",
        "    if tf.shape(X)[0] == tf.shape(Y)[0]:\n",
        "        test_idxs = tf.range(start=0, limit=tf.shape(X)[0], dtype=tf.int32)\n",
        "        shuffled_test_idxs = tf.random.shuffle(test_idxs)\n",
        "        return (tf.gather(X, shuffled_test_idxs), tf.gather(Y, shuffled_test_idxs))\n",
        "    else:\n",
        "        raise ValueError(f\"0-dimension has to be the same {tf.shape(X)[0]} != {tf.shape(Y)[0]}\")\n",
        "\n",
        "def reverse_one_hot(Y_input):\n",
        "    return np.apply_along_axis(np.argmax, 1, Y_input) + 1\n",
        "\n",
        "def getNpArrayFromH5(hf_Data):\n",
        "    X_train = hf_Data['Train_Data']  # Get train set\n",
        "    X_train = np.array(X_train)\n",
        "    Y_train = hf_Data['Label']  # Get train label\n",
        "    Y_train = np.array(Y_train)\n",
        "    return X_train, Y_train\n",
        "\n",
        "def get_confusion_matrix_classification(model, X, Y_true):\n",
        "    y_pred = model.predict(X)\n",
        "    y_true = np.apply_along_axis(np.argmax, 1, Y_true)\n",
        "    y_pred = np.apply_along_axis(np.argmax, 1, y_pred)\n",
        "    return (confusion_matrix(y_true, y_pred), y_pred, y_true)\n",
        "\n",
        "def misclass_perc_to_weight(input_confusion, add_base=True, func=None):\n",
        "    perc_misclassified = 1.0 - np.array([ input_confusion[x,x] for x in np.arange(input_confusion.shape[0]).tolist() ])/input_confusion.sum(axis=1)\n",
        "    \n",
        "    base_val = min(perc_misclassified[perc_misclassified>0.0])\n",
        "    if add_base:        \n",
        "        perc_misclassified = perc_misclassified + base_val\n",
        "    \n",
        "    perc_misclassified = [ x/base_val for x in perc_misclassified]\n",
        "    return dict([ (idx, func(perc_val)) if func is not None else (idx, perc_val) for idx, perc_val in enumerate(perc_misclassified) ])\n",
        "\n",
        "def prf(model,xtest, ytest):\n",
        "  y_pred = np.apply_along_axis(np.argmax, 1, model.predict(xtest))\n",
        "  y_true = np.apply_along_axis(np.argmax, 1, ytest)\n",
        "  return precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
        "\n",
        "def get_sp_pr_rc_f1(model,xtest, ytest):  \n",
        "    y_pred = np.apply_along_axis(np.argmax, 1, model.predict(xtest))\n",
        "    y_true = np.apply_along_axis(np.argmax, 1, ytest)\n",
        "    cmres = ConfusionMatrix(actual_vector=y_true,predict_vector=y_pred)\n",
        "    pr, rc, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")   \n",
        "    return cmres.TNR_Macro, pr, rc, f1\n",
        " \n",
        "def get_sp_pr_rc_f1_acc(model,xtest, ytest):  \n",
        "    spec, pr, rc, f1 = get_sp_pr_rc_f1(model,xtest, ytest)\n",
        "    acc = model.evaluate(xtest,ytest)[-1]        \n",
        "    return spec, pr, rc, f1, acc\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dd-0iNNhk3t"
      },
      "source": [
        "def plot_history(history):\n",
        "    acc_keys = [k for k in history.history.keys() if k in ('accuracy', 'val_accuracy')]\n",
        "    loss_keys = [k for k in history.history.keys() if not k in acc_keys]\n",
        "    for k, v in history.history.items():\n",
        "        if k in acc_keys:\n",
        "            plt.figure(1)\n",
        "            plt.plot(v)\n",
        "        else:\n",
        "            plt.figure(2)\n",
        "            plt.plot(v)\n",
        "    plt.figure(1)\n",
        "    plt.title('Accuracy vs. epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(acc_keys, loc='lower right')\n",
        "    plt.figure(2)\n",
        "    plt.title('Loss vs. epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(loss_keys, loc='upper right')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def get_layer_by_name(layers, name, return_first=True):\n",
        "    matching_named_layers = [l for l in layers if l.name == name]\n",
        "    if not matching_named_layers:\n",
        "        return None\n",
        "    return matching_named_layers[0] if return_first else matching_named_layers\n",
        "\n",
        "\n",
        "def make_dir_if_not_exist(used_path):\n",
        "    if not os.path.isdir(used_path):\n",
        "        try:\n",
        "            os.mkdir(used_path)\n",
        "        except OSError as exc:\n",
        "            if exc.errno != errno.EEXIST:\n",
        "                raise exc\n",
        "            else:\n",
        "                raise ValueError(f'{used_path} directoy cannot be created because its parent directory does not exist.')\n",
        "\n",
        "\n",
        "def source_model(model_func, model_name, input_shape):\n",
        "    m = None\n",
        "    if isinstance(model_func, tf.keras.models.Model):\n",
        "        m = model_func\n",
        "        m._name = model_name\n",
        "    else:\n",
        "        m = model_func(model_name, input_shape)\n",
        "    return m\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTv646yChscq"
      },
      "source": [
        "\n",
        "\n",
        "def compile_and_fit_model_with_tb(model_func,\n",
        "                                  model_name,\n",
        "                                  input_shape,\n",
        "                                  X_train,\n",
        "                                  Y_train,\n",
        "                                  save_every_epoch=True,\n",
        "                                  save_final=False,\n",
        "                                  **kwargs):\n",
        "    m = None\n",
        "    if isinstance(model_func, tf.keras.models.Model):\n",
        "        m = model_func\n",
        "        m._name = model_name\n",
        "    else:\n",
        "        m = model_func(model_name, input_shape)\n",
        "    tb_callback = TensorBoard(log_dir=f'{m.name}_logs', histogram_freq=kwargs.pop(\"histogram_freq\", 1))\n",
        "    if save_every_epoch:\n",
        "        tb_callback.append(ModelCheckpoint(f'{m.name}' + '_model_{epoch:03d}_{val_accuracy:0.2f}'))\n",
        "    m.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    history = m.fit(X_train, Y_train, callbacks=[tb_callback], verbose=2, **kwargs)\n",
        "    if save_final:\n",
        "        make_dir_if_not_exist(model_name)\n",
        "        m.save(f\"{m.name}_saved_model_after_fit\")  # Save the model\n",
        "    return (m, history)\n",
        "    # m.save(f\"{m.name}_Tenth_Fold_New_Model_500_8\") #Save the model\n",
        "\n",
        "\n",
        "def compile_model_and_fit_with_custom_loop(model_func,\n",
        "                                           model_name,\n",
        "                                           input_shape,\n",
        "                                           X_train,\n",
        "                                           Y_train,\n",
        "                                           **kwargs):\n",
        "    make_dir_if_not_exist(model_name)\n",
        "    m = None\n",
        "    if isinstance(model_func, tf.keras.models.Model):\n",
        "        m = model_func\n",
        "        m._name = model_name\n",
        "    else:\n",
        "        m = model_func(model_name, input_shape)\n",
        "\n",
        "    train_writer = create_file_writer(f'{m.name}_logs/train/')\n",
        "    test_writer = create_file_writer(f'{m.name}_logs/test/')\n",
        "    train_step = test_step = 0\n",
        "\n",
        "    acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    num_epochs = kwargs.get(\"epochs\", 10)\n",
        "\n",
        "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "    BATCH_SIZE = kwargs.get(\"batch_size\", 32)\n",
        "    X_test, Y_test = kwargs.get(\"validation_data\", (None, None))\n",
        "    if X_test is None:\n",
        "        raise ValueError(\"Missing X validation data\")\n",
        "    if Y_test is None:\n",
        "        raise ValueError(\"Missing Y validation data\")\n",
        "\n",
        "    train_dataset_tf = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
        "    train_dataset_tf = train_dataset_tf.batch(BATCH_SIZE)\n",
        "    train_dataset_tf = train_dataset_tf.prefetch(AUTOTUNE)\n",
        "\n",
        "    test_dataset_tf = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n",
        "    test_dataset_tf = train_dataset_tf.batch(BATCH_SIZE)\n",
        "    test_dataset_tf = train_dataset_tf.prefetch(AUTOTUNE)\n",
        "\n",
        "    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Iterate through training set\n",
        "        for batch_idx, (x, y) in enumerate(train_dataset_tf):\n",
        "            with tf.GradientTape() as tape:\n",
        "                y_pred = m(x, training=True)\n",
        "                loss = loss_fn(y, y_pred)\n",
        "\n",
        "            gradients = tape.gradient(loss, m.trainable_weights)\n",
        "            optimizer.apply_gradients(zip(gradients, m.trainable_weights))\n",
        "            acc_metric.update_state(y, y_pred)\n",
        "\n",
        "            with train_writer.as_default():\n",
        "                tf.summary.scalar(\"Loss\", loss, step=train_step)\n",
        "                tf.summary.scalar(\n",
        "                    \"Accuracy\", acc_metric.result(), step=train_step,\n",
        "                )\n",
        "                train_step += 1\n",
        "        # Reset accuracy in between epochs (and for testing and test)\n",
        "        acc_metric.reset_states()\n",
        "        # Iterate through test set\n",
        "        for batch_idx, (x, y) in enumerate(test_dataset_tf):\n",
        "            y_pred = m(x, training=False)\n",
        "            loss = loss_fn(y, y_pred)\n",
        "            acc_metric.update_state(y, y_pred)\n",
        "            with test_writer.as_default():\n",
        "                tf.summary.scalar(\"Loss\", loss, step=test_step)\n",
        "                tf.summary.scalar(\n",
        "                    \"Accuracy\", acc_metric.result(), step=test_step,\n",
        "                )\n",
        "                test_step += 1\n",
        "\n",
        "        acc_metric.reset_states()  # Reset accuracy in between epochs (and for testing and test)\n",
        "\n",
        "    return m\n",
        "\n",
        "\n",
        "\n",
        "def run_mirrored_strategy(model_func, base_batch_size, nepochs, x_train, y_train, x_test, y_test, **kwargs):\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "    with strategy.scope():\n",
        "        model = model_func()\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(),\n",
        "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "            metrics=tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "        )\n",
        "    batch_size_mirr_strat = base_batch_size * strategy.num_replicas_in_sync\n",
        "    history = model.fit(x_train, y_train, epochs=nepochs, batch_size=batch_size_mirr_strat,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        **kwargs)\n",
        "    return model, history\n",
        "\n",
        "\n",
        "def sparse_setdiff(a1, a2):\n",
        "    a1a = a1.reshape(a1.shape[0], -1)\n",
        "    a2a = a2.reshape(a2.shape[0], -1)\n",
        "    spa2a = [np.where(x)[0].tolist() for x in a2a]\n",
        "    spa1a = [np.where(x)[0].tolist() for x in a1a]\n",
        "    idxs_to_keep = []\n",
        "    for idx, sample in enumerate(spa1a):\n",
        "        try:\n",
        "            spa2a.index(sample)\n",
        "        except ValueError:\n",
        "            # not in list\n",
        "            idxs_to_keep.append(idx)\n",
        "    return a1[idxs_to_keep], idxs_to_keep\n",
        "\n",
        "\n",
        "\n",
        "def reinitialize_weights(model):\n",
        "    for ix, layer in enumerate(model.layers):\n",
        "        if hasattr(model.layers[ix], 'kernel_initializer') and hasattr(model.layers[ix], 'bias_initializer'):\n",
        "            weight_initializer = model.layers[ix].kernel_initializer\n",
        "            bias_initializer = model.layers[ix].bias_initializer\n",
        "    \n",
        "            old_weights, old_biases = model.layers[ix].get_weights()\n",
        "    \n",
        "            model.layers[ix].set_weights([\n",
        "                weight_initializer(shape=old_weights.shape),\n",
        "                bias_initializer(shape=len(old_biases))])            \n",
        "    return model\n",
        "\n",
        "def reverse_tensor(X):\n",
        "    return tf.gather(X, tf.reverse(tf.range(start=0, limit=tf.shape(X)[0], dtype=tf.int32),(0,)) )\n",
        "\n",
        "def get_combined_features_from_models(\n",
        "    \n",
        "        to_combine,\n",
        "        X_train, Y_train,\n",
        "        X_test, Y_test,\n",
        "        reverse_one_hot=False,\n",
        "        normalize_X_func=None):\n",
        "    \n",
        "    models = []\n",
        "    models_dict = {}\n",
        "    X_trains_out = []\n",
        "    X_test_out = []\n",
        "    XY_dict = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: None))))\n",
        "\n",
        "    models_have_different_inputs = isinstance(Y_train,list)\n",
        "\n",
        "    if reverse_one_hot:\n",
        "        if models_have_different_inputs:\n",
        "            Y_train_new = np.apply_along_axis(np.argmax, 1, Y_train) + 1\n",
        "            Y_test_new = np.apply_along_axis(np.argmax, 1, Y_test) + 1\n",
        "        else:\n",
        "            Y_train_new = [ np.apply_along_axis(np.argmax, 1, y_train) + 1 for y_train in Y_train ]  \n",
        "            Y_test_new = [ np.apply_along_axis(np.argmax, 1, y_test) + 1 for y_test in Y_test ]              \n",
        "    else:\n",
        "        if models_have_different_inputs:\n",
        "            Y_train_new = Y_train.copy()\n",
        "            Y_test_new = Y_test.copy()\n",
        "        else:\n",
        "            Y_train_new = [ y_train.copy() for y_train in Y_train ] \n",
        "            Y_test_new = [ y_test.copy() for y_test in Y_train ] \n",
        "            \n",
        "\n",
        "    extraction_counter =0\n",
        "    for model_file_name, layer_name, kwargs in to_combine:\n",
        "        model_here = None\n",
        "        if isinstance(model_file_name, tf.keras.models.Model):\n",
        "            model_here = model_file_name\n",
        "            model_file_name = model_here.name\n",
        "        else:\n",
        "            if model_file_name in models_dict.keys():\n",
        "                model_here = models_dict[model_file_name]\n",
        "            else:\n",
        "                model_here = tf.keras.models.load_model(model_file_name,\n",
        "                                                        **kwargs) if kwargs is not None else tf.keras.models.load_model \\\n",
        "                    (model_file_name)\n",
        "\n",
        "        features_model = Model(model_here.input,\n",
        "                               get_layer_by_name(model_here.layers, layer_name).output)\n",
        "        \n",
        "        if normalize_X_func is None:\n",
        "            X_trains_out.append(np.array(features_model.predict(X_train if not models_have_different_inputs else X_train[extraction_counter]), dtype='float64'))\n",
        "            X_test_out.append(np.array(features_model.predict(X_test if not models_have_different_inputs else X_test[extraction_counter]), dtype='float64'))\n",
        "        else:\n",
        "            X_trains_out.append(np.array(normalize_X_func(features_model.predict(X_train if not models_have_different_inputs else X_train[extraction_counter])), dtype='float64'))\n",
        "            X_test_out.append(np.array(normalize_X_func(features_model.predict(X_test if not models_have_different_inputs else X_test[extraction_counter])), dtype='float64'))\n",
        "        XY_dict[model_file_name][layer_name]['Train']['X'] = X_trains_out[-1]\n",
        "        XY_dict[model_file_name][layer_name]['Test']['X'] = X_test_out[-1]\n",
        "        XY_dict[model_file_name][layer_name]['Train']['Y'] = Y_train_new\n",
        "        XY_dict[model_file_name][layer_name]['Test']['Y'] = Y_test_new\n",
        "        models.append(((model_file_name, layer_name), (model_here, features_model)))\n",
        "        models_dict[model_file_name] = model_here\n",
        "        extraction_counter += 1\n",
        "\n",
        "    X_train_new = np.concatenate(tuple(X_trains_out), axis=1)\n",
        "    X_test_new = np.concatenate(tuple(X_test_out), axis=1)\n",
        "\n",
        "    data_train = (X_train_new, Y_train_new)\n",
        "    data_test = (X_test_new, Y_test_new)\n",
        "\n",
        "    return models, data_train, data_test, XY_dict\n",
        "\n",
        "class SaveBestOverCombinedThresholds(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def __init__(self, colab_download = False, observed_values = [ ('accuracy',0.9) ] ):\n",
        "        self.thresholds = dict(observed_values)\n",
        "        self.last_best_values = dict([ (obs_name, np.nan) for obs_name in self.thresholds.keys()] )\n",
        "        self.colab_download = colab_download\n",
        "        \n",
        "    def on_epoch_end(self, epoch, logs=None):        \n",
        "        register = None\n",
        "        for k,v in self.thresholds.items():\n",
        "            if k not in logs.keys():\n",
        "                raise ValueError(f\"{k} not found in logs\")\n",
        "            passes_threshold = logs[k] > self.thresholds[k]                 \n",
        "            register = passes_threshold if register is None else (register and passes_threshold)\n",
        "        \n",
        "        if register:\n",
        "            for k,v in self.thresholds.items():\n",
        "                if np.isnan(self.last_best_values[k]):\n",
        "                    self.last_best_values[k] = logs[k]\n",
        "                else:\n",
        "                    if logs[k] < self.last_best_values[k]:\n",
        "                        register = False\n",
        "                        break\n",
        "            if register:\n",
        "                for k,v in self.thresholds.items():\n",
        "                    self.last_best_values[k] = logs[k]\n",
        "                base_name = f'{self.model.name}_epoch_{str(epoch)}_{\"_\".join([\"{}_{:.3f}\".format(k,v) for k,v in self.last_best_values.items()])}'\n",
        "                self.model.save(f'{base_name}.h5')                \n",
        "                history_df = pd.DataFrame(self.model.history.history) \n",
        "                history_df.to_csv(f'{base_name}_history.csv',header=True, index=False)\n",
        "                \n",
        "                if self.colab_download:\n",
        "                    from google.colab import files\n",
        "                    files.download(f'{base_name}.h5')\n",
        "                    files.download(f'{base_name}_history.csv')\n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08mDn9XtXtIp"
      },
      "source": [
        "# data extraction\n",
        "def getE2eData13(shuffle=False):\n",
        "    hf_Train = h5.File(f'{data_location}/e2e_Train_Data_1000.h5', 'r')\n",
        "    hf_Test = h5.File(f'{data_location}/e2e_Test_Data_1000.h5', 'r')\n",
        "\n",
        "    X_train, Y_train = getNpArrayFromH5(hf_Train)\n",
        "    X_test, Y_test = getNpArrayFromH5(hf_Test)\n",
        "    Y_train = to_categorical(Y_train, 13)  # Process the label of tain\n",
        "    Y_test = to_categorical(Y_test, 13)  # Process the label of te\n",
        "\n",
        "    if shuffle:\n",
        "        X_train, Y_train = coShuffled_vectors(X_train, Y_train)\n",
        "        X_test, Y_test = coShuffled_vectors(X_test, Y_test)\n",
        "\n",
        "    hf_Val = h5.File(f'{data_location}/e2e_Val_Data_1000.h5', 'r')\n",
        "    X_validation, Y_validation = getNpArrayFromH5(hf_Val)\n",
        "    Y_validation = to_categorical(Y_validation, 13)  \n",
        "\n",
        "    return X_train, Y_train, X_test, Y_test, X_validation, Y_validation\n",
        "\n",
        "\n",
        "def getE2eDataJustSecondary(shuffle=False,isColab=False):\n",
        "    hf_Train = h5.File(f'{data_location}/e2e_Train_just_Secondary_Data_1000.h5', 'r')\n",
        "    hf_Test = h5.File(f'{data_location}/e2e_Test_just_Secondary_Data_1000.h5', 'r')\n",
        "\n",
        "    X_train, Y_train = getNpArrayFromH5(hf_Train)\n",
        "    X_test, Y_test = getNpArrayFromH5(hf_Test)\n",
        "    \n",
        "    Y_train = to_categorical(Y_train, Y_test.shape[-1])  \n",
        "    Y_test = to_categorical(Y_test, Y_test.shape[-1])  \n",
        "\n",
        "    if shuffle:\n",
        "        X_train, Y_train = coShuffled_vectors(X_train, Y_train)\n",
        "        X_test, Y_test = coShuffled_vectors(X_test, Y_test)\n",
        "\n",
        "    hf_Val = h5.File(f'{data_location}/e2e_Val_just_Secondary_Data_1000.h5', 'r')\n",
        "    \n",
        "    X_validation, Y_validation = getNpArrayFromH5(hf_Val)\n",
        "    Y_validation = to_categorical(Y_validation, Y_test.shape[-1])  # Process the label of tain\n",
        "\n",
        "    return X_train, Y_train, X_test, Y_test, X_validation, Y_validation\n",
        "\n",
        "\n",
        "def getTest12Data():\n",
        "    hf_Test = h5.File(f'{data_location}/e2e_Test_Data_1000_12classes.h5', 'r')\n",
        "    X_test, Y_test = getNpArrayFromH5(hf_Test)\n",
        "    Y_test = to_categorical(Y_test, 13)  # Process the label of te\n",
        "    return X_test, Y_test\n",
        "\n",
        "\n",
        "def get88KData():\n",
        "    hf_Train = h5.File(f'{data_location}/e2e_Train_Data_1000_88.h5', 'r')\n",
        "    hf_Test = h5.File(f'{data_location}/e2e_Test_Data_1000_88.h5', 'r')\n",
        "\n",
        "    X_train, Y_train = getNpArrayFromH5(hf_Train)\n",
        "    X_test, Y_test = getNpArrayFromH5(hf_Test)\n",
        "    Y_train = to_categorical(Y_train, Y_test.shape[-1])  # Process the label of tain\n",
        "    Y_test = to_categorical(Y_test, Y_test.shape[-1])  # Process the label of te\n",
        "\n",
        "    hf_Val = h5.File(f'{data_location}/e2e_Val_Data_1000_88.h5', 'r') \n",
        "    X_validation, Y_validation = getNpArrayFromH5(hf_Val)\n",
        "    Y_validation = to_categorical(Y_validation, Y_test.shape[-1])  # Process the label of tain\n",
        "\n",
        "    return X_train, Y_train, X_test, Y_test, X_validation, Y_validation\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNo_peRzfXNP"
      },
      "source": [
        "A function that will take a model construction function (with _model_name_ and _input_shape_ arguments), train and fit it using the supplied data and callbacks. The _kwargs_ are supplied to the model fit function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_dqIA0noEoo"
      },
      "source": [
        "from tensorflow.keras.callbacks import CSVLogger\n",
        "import datetime\n",
        "\n",
        "def compile_and_fit_model_basic(  model_func,\n",
        "                                  model_name,\n",
        "                                  input_shape,\n",
        "                                  X_train,\n",
        "                                  Y_train,\n",
        "                                  save_max_epoch=True,\n",
        "                                  save_final=False,\n",
        "                                  patience_count = None,\n",
        "                                  early_stopping_obs = 'val_sparse_categorical_accuracy',\n",
        "                                  log_history = True,\n",
        "                                  verbose_level = 0,\n",
        "                                  **kwargs):\n",
        "    m = None\n",
        "    if isinstance(model_func, tf.keras.models.Model):\n",
        "        m = model_func\n",
        "        m._name = model_name\n",
        "    else:\n",
        "        m = model_func(model_name, input_shape)\n",
        "      \n",
        "    if 'validation_data' not in kwargs.keys() and 'val_' in early_stopping_obs:\n",
        "        early_stopping_obs = early_stopping_obs.replace('val_','')\n",
        "\n",
        "    callbacks_used = []\n",
        "    if save_max_epoch:\n",
        "        callbacks_used.append(ModelCheckpoint(f'/content/drive/MyDrive/data_papers/{paper_name}/model_checkpoints/{m.name}' + '_model_{epoch:03d}_{accuracy:0.3f}',\n",
        "                                              save_weights_only=False,\n",
        "                                              monitor = early_stopping_obs,\n",
        "                                              mode='max',\n",
        "                                              save_best_only=True))\n",
        "    if patience_count is not None:\n",
        "        callbacks_used.append(tf.keras.callbacks.EarlyStopping(monitor=early_stopping_obs, patience=patience_count))\n",
        "\n",
        "    if log_history:\n",
        "        callbacks_used.append(tf.keras.callbacks.CSVLogger(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_history/history_log_{model_name}_{datetime.date.today().strftime('%Y%m%d')}.csv\", append=True))\n",
        "\n",
        "    m.compile(loss=tf.keras.losses.CategoricalCrossentropy(), \n",
        "              optimizer=tf.keras.optimizers.Adam(), \n",
        "              metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
        "    \n",
        "    history = m.fit(X_train, Y_train, callbacks=callbacks_used, verbose=verbose_level, **kwargs)\n",
        "    if save_final:\n",
        "        make_dir_if_not_exist(model_name)\n",
        "        m.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/{m.name}_saved_model_after_fit\")  # Save the model\n",
        "    return (m, history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J212e-sSgQEs"
      },
      "source": [
        "A function that given a model or model directory create a new model up to the _layer_name_, then write the features matching the supplied _X_ and _Y_ as numpy arrays to google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxMillEBxs5H"
      },
      "source": [
        "def write_features_from_models(\n",
        "        model_entry,\n",
        "        layer_name,\n",
        "        X_input, Y_input,\n",
        "        reverse_one_hot=False,\n",
        "        normalize_X_func=None,\n",
        "        dataset_id = \"NA\",\n",
        "        **kwargs):\n",
        "  \n",
        "    Y_new = None\n",
        "    X_new = None\n",
        "    if reverse_one_hot:\n",
        "        Y_new = np.apply_along_axis(np.argmax, 1, Y_input) + 1\n",
        "    else:\n",
        "        Y_new = Y_input.copy()\n",
        "\n",
        "    model_here = None\n",
        "    if isinstance(model_entry, tf.keras.models.Model):\n",
        "        model_here = model_entry\n",
        "        model_file_name = model_here.name\n",
        "    else:\n",
        "        model_here = tf.keras.models.load_model(model_entry,**kwargs) \n",
        "\n",
        "    features_model = Model(model_here.input,\n",
        "                            get_layer_by_name(model_here.layers, layer_name).output)\n",
        "    if normalize_X_func is None:\n",
        "        X_new = np.array(features_model.predict(X_input), dtype='float64')\n",
        "    else:\n",
        "        X_new = np.array(normalize_X_func(features_model.predict(X_input)), dtype='float64')\n",
        "\n",
        "    np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features/{model_here.name}_features_{layer_name}_{dataset_id}_X\", X_new, \n",
        "               allow_pickle=True, \n",
        "               fix_imports=True)\n",
        "    np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features/{model_here.name}_features_{layer_name}_{dataset_id}_Y\", Y_new, \n",
        "               allow_pickle=True, \n",
        "               fix_imports=True)\n",
        "\n",
        "\n",
        "def write_features_from_models_idxCnt(\n",
        "        model_entry,\n",
        "        idxCnt,\n",
        "        X_input, Y_input,\n",
        "        reverse_one_hot=False,\n",
        "        normalize_X_func=None,\n",
        "        dataset_id = \"NA\",\n",
        "        **kwargs):\n",
        "  \n",
        "    Y_new = None\n",
        "    X_new = None\n",
        "    if reverse_one_hot:\n",
        "        Y_new = np.apply_along_axis(np.argmax, 1, Y_input) + 1\n",
        "    else:\n",
        "        Y_new = Y_input.copy()\n",
        "\n",
        "    model_here = None\n",
        "    if isinstance(model_entry, tf.keras.models.Model):\n",
        "        model_here = model_entry\n",
        "        model_file_name = model_here.name\n",
        "    else:\n",
        "        model_here = tf.keras.models.load_model(model_entry,**kwargs) \n",
        "\n",
        "    features_model = Model(model_here.input,\n",
        "                            model_here.layers[idxCnt].output)\n",
        "    if normalize_X_func is None:\n",
        "        X_new = np.array(features_model.predict(X_input), dtype='float64')\n",
        "    else:\n",
        "        X_new = np.array(normalize_X_func(features_model.predict(X_input)), dtype='float64')\n",
        "\n",
        "    np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features/{model_here.name}_features_{model_here.layers[idxCnt].name}_{dataset_id}_X\", X_new, \n",
        "               allow_pickle=True, \n",
        "               fix_imports=True)\n",
        "    np.save(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_features/{model_here.name}_features_{model_here.layers[idxCnt].name}_{dataset_id}_Y\", Y_new, \n",
        "               allow_pickle=True, \n",
        "               fix_imports=True)               "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2O5XMTr2s5m"
      },
      "source": [
        "Some functions to get scores on the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhPJFtuzujdC"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "import re\n",
        "\n",
        "def get_confusion_matrix_classification(model, X, Y_true):\n",
        "    y_pred = model.predict(X)\n",
        "    y_true = np.apply_along_axis(np.argmax, 1, Y_true)\n",
        "    y_pred = np.apply_along_axis(np.argmax, 1, y_pred)\n",
        "    return (confusion_matrix(y_true, y_pred), y_pred, y_true)\n",
        "\n",
        "def construct_confusion_matrix(X, Y_true, Y_pred):\n",
        "    y_true = Y_true\n",
        "    y_pred = np.apply_along_axis(np.argmax, 1, Y_pred)\n",
        "    return (confusion_matrix(y_true, y_pred), y_pred, y_true)\n",
        "\n",
        "def pr_rc_f1_acc_from_supplied(y_pred, y_true):  \n",
        "    pr, rc, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")   \n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    return pr, rc, f1, acc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVz_etHKO89T"
      },
      "source": [
        "import re\n",
        "import os\n",
        "\n",
        "def dir_has_file_with_regex(dir_name, regex_string):\n",
        "  filenames = [ f\"{dir_name}/{dir_entry.name}\" for dir_entry in os.scandir(dir_name) if os.path.isfile(f\"{dir_name}/{dir_entry.name}\") ]   \n",
        "  filenames = [ fn for fn in filenames if re.match(regex_string, fn, re.IGNORECASE) ]\n",
        "  return filenames\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiA0TTr4VR2Z"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "from tensorflow.keras.models import Model, Sequential, load_model\n",
        "from tensorflow.keras.layers import  Dense, Flatten, Activation, Dropout, Embedding, Conv1D, Conv2D, MaxPooling2D, MaxPooling1D, Concatenate, BatchNormalization, GaussianNoise\n",
        "from tensorflow.keras.layers import LSTM, TimeDistributed, Permute, Reshape, Lambda, RepeatVector, Input, Multiply, SimpleRNN, GRU, LeakyReLU\n",
        "from keras_self_attention import SeqSelfAttention, SeqWeightedAttention\n",
        "import site\n",
        "import pandas as pd\n",
        "import os\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "import datetime\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9DSXcStXi5C"
      },
      "source": [
        "X_train_1000e, Y_train_1000e, X_test_1000e, Y_test_1000e, X_val_1000e, Y_val_1000e = getE2eData13()\n",
        "X_new_train = np.concatenate( (X_train_1000e, X_val_1000e), axis=0 )\n",
        "Y_new_train = np.concatenate( (Y_train_1000e, Y_val_1000e), axis=0 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7HAq0FaDTAA"
      },
      "source": [
        "# Show individual CNN/RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BHJYLeQDTAA",
        "outputId": "f571bfd5-7b8e-4603-e749-1087dd83216a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['temp',\n",
              " 'individual_cnn_summary_20211206214644.npy',\n",
              " 'individual_rnn_summary_20211207160708.npy',\n",
              " 'ensemble_rnn_test_results_20211207213341.csv',\n",
              " 'ensemble_cnn_test_results_20211207215955.csv',\n",
              " 'cnn_rnn_ensemble_contour_data_20211207233449.csv',\n",
              " 'cnn_rnn_ensemble_contour_data_20211207235706.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ],
      "source": [
        "os.listdir(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indiv_cnns = np.load(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/individual_cnn_summary_20211206214644.npy\")\n",
        "indiv_rnns = np.load(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/individual_rnn_summary_20211207160708.npy\")"
      ],
      "metadata": {
        "id": "3mq4H5RMDhKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indiv_cnns.mean(axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7_JgmggDn9c",
        "outputId": "f4c492b2-910f-4658-88b5-e248540f3b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.95071833, 0.94772727, 0.947997  , 0.94772727])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indiv_rnns.mean(axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lzsz2CkPDpED",
        "outputId": "c61ce898-45fd-40c1-94e8-71c7e5e70cca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.77462007, 0.77233877, 0.75532836, 0.77233877])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indiv_rnns.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a5x0ROVEF7I",
        "outputId": "cbfc0eae-6701-44b8-df35-72a5c143f003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indiv_rnns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lMyeihJEHMq",
        "outputId": "87d10751-9957-43bf-da48-0214f370dea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.81086121, 0.78088578, 0.7751795 , 0.78088578],\n",
              "       [0.80863864, 0.78554779, 0.77521623, 0.78554779],\n",
              "       [0.91999956, 0.91491841, 0.91281406, 0.91491841],\n",
              "       [0.86983951, 0.86829837, 0.86383539, 0.86829837],\n",
              "       [0.92132802, 0.91025641, 0.91037113, 0.91025641],\n",
              "       [0.92490678, 0.91958042, 0.918416  , 0.91958042],\n",
              "       [0.18672943, 0.23776224, 0.12371224, 0.23776224],\n",
              "       [0.93097164, 0.92657343, 0.92717165, 0.92657343],\n",
              "       [0.86647343, 0.85198135, 0.84579051, 0.85198135],\n",
              "       [0.25692532, 0.29020979, 0.23698688, 0.29020979],\n",
              "       [0.89440603, 0.88111888, 0.87574705, 0.88111888],\n",
              "       [0.90436124, 0.9009324 , 0.89869963, 0.9009324 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "u1Ucbt1XEIWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iyyimVKOpEl"
      },
      "source": [
        "# Show ensemble CNN/RNN/CNN&RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30ff4e9f-b124-42cd-d42c-d70c6e0f21a0",
        "id": "WTOdzo82OpEl"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['temp',\n",
              " 'individual_cnn_summary_20211206214644.npy',\n",
              " 'individual_rnn_summary_20211207160708.npy',\n",
              " 'ensemble_rnn_test_results_20211207213341.csv',\n",
              " 'ensemble_cnn_test_results_20211207215955.csv',\n",
              " 'cnn_rnn_ensemble_contour_data_20211207233449.csv',\n",
              " 'cnn_rnn_ensemble_contour_data_20211207235706.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "os.listdir(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_rnns = pd.read_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/ensemble_rnn_test_results_20211207213341.csv\")\n",
        "ensemble_cnns = pd.read_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/ensemble_cnn_test_results_20211207215955.csv\")\n",
        "ensemble_cnnrnnss = pd.read_csv(f\"/content/drive/MyDrive/data_papers/{paper_name}/summary_results/cnn_rnn_ensemble_contour_data_20211207235706.csv\")"
      ],
      "metadata": {
        "id": "VF-kIF1TOpEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_cnns[\"CNN%\"] = np.select([ensemble_cnns.Type==\"CNN256\",ensemble_cnns.Type==\"RNN\"],[1.0,0.0])\n",
        "ensemble_rnns[\"CNN%\"] = np.select([ensemble_rnns.Type==\"CNN256\",ensemble_rnns.Type==\"RNN\"],[1.0,0.0])\n"
      ],
      "metadata": {
        "id": "Gxu7gQpKO65H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_cnnrnnss = ensemble_cnnrnnss.drop_duplicates()\n",
        "ensemble_cnnrnnss[\"NumOfModels\"] = ensemble_cnnrnnss.NumOfA + ensemble_cnnrnnss.NumOfB\n",
        "ensemble_cnnrnnss[\"Type\"] = \"CNN_RNN\"\n",
        "ensemble_cnnrnnss[\"CNN%\"] = ensemble_cnnrnnss.NumOfA/ensemble_cnnrnnss[\"NumOfModels\"]"
      ],
      "metadata": {
        "id": "UIgqX-E-O78B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ensemble_rnns"
      ],
      "metadata": {
        "id": "BPP9t0-sQ5gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_data = pd.concat([ ensemble_rnns[[\"Type\",\"Data\",\"NumOfModels\",\"CNN%\",\"Pr\",\"Rc\",\"F1\",\"Acc\"]],\n",
        "            ensemble_cnns[[\"Type\",\"Data\",\"NumOfModels\",\"CNN%\",\"Pr\",\"Rc\",\"F1\",\"Acc\"]],\n",
        "            ensemble_cnnrnnss[[\"Type\",\"Data\",\"NumOfModels\",\"CNN%\",\"Pr\",\"Rc\",\"F1\",\"Acc\"]] ]).drop_duplicates()"
      ],
      "metadata": {
        "id": "fq_RxDTgPnWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_data.loc[(ensemble_data[\"CNN%\"] >= 1.0) & (ensemble_data[\"NumOfModels\"]>=20),[\"NumOfModels\",\"Pr\",\"Rc\",\"F1\",\"Acc\"]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "zWtcesfvRiAM",
        "outputId": "c9fa0a37-8749-4731-ef28-8d3f0d4041ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NumOfModels</th>\n",
              "      <th>Pr</th>\n",
              "      <th>Rc</th>\n",
              "      <th>F1</th>\n",
              "      <th>Acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>20</td>\n",
              "      <td>0.960386</td>\n",
              "      <td>0.959207</td>\n",
              "      <td>0.959405</td>\n",
              "      <td>0.959207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>20</td>\n",
              "      <td>0.956702</td>\n",
              "      <td>0.955711</td>\n",
              "      <td>0.955703</td>\n",
              "      <td>0.955711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>22</td>\n",
              "      <td>0.957807</td>\n",
              "      <td>0.956876</td>\n",
              "      <td>0.957092</td>\n",
              "      <td>0.956876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>22</td>\n",
              "      <td>0.963952</td>\n",
              "      <td>0.962704</td>\n",
              "      <td>0.962990</td>\n",
              "      <td>0.962704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>25</td>\n",
              "      <td>0.964572</td>\n",
              "      <td>0.963869</td>\n",
              "      <td>0.964025</td>\n",
              "      <td>0.963869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>25</td>\n",
              "      <td>0.963641</td>\n",
              "      <td>0.962704</td>\n",
              "      <td>0.962919</td>\n",
              "      <td>0.962704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>30</td>\n",
              "      <td>0.963598</td>\n",
              "      <td>0.962704</td>\n",
              "      <td>0.962878</td>\n",
              "      <td>0.962704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>30</td>\n",
              "      <td>0.960497</td>\n",
              "      <td>0.959207</td>\n",
              "      <td>0.959534</td>\n",
              "      <td>0.959207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>35</td>\n",
              "      <td>0.961246</td>\n",
              "      <td>0.960373</td>\n",
              "      <td>0.960547</td>\n",
              "      <td>0.960373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>35</td>\n",
              "      <td>0.965308</td>\n",
              "      <td>0.963869</td>\n",
              "      <td>0.964177</td>\n",
              "      <td>0.963869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>40</td>\n",
              "      <td>0.960164</td>\n",
              "      <td>0.959207</td>\n",
              "      <td>0.959395</td>\n",
              "      <td>0.959207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>40</td>\n",
              "      <td>0.959421</td>\n",
              "      <td>0.958042</td>\n",
              "      <td>0.958281</td>\n",
              "      <td>0.958042</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    NumOfModels        Pr        Rc        F1       Acc\n",
              "70           20  0.960386  0.959207  0.959405  0.959207\n",
              "71           20  0.956702  0.955711  0.955703  0.955711\n",
              "72           22  0.957807  0.956876  0.957092  0.956876\n",
              "73           22  0.963952  0.962704  0.962990  0.962704\n",
              "74           25  0.964572  0.963869  0.964025  0.963869\n",
              "75           25  0.963641  0.962704  0.962919  0.962704\n",
              "76           30  0.963598  0.962704  0.962878  0.962704\n",
              "77           30  0.960497  0.959207  0.959534  0.959207\n",
              "78           35  0.961246  0.960373  0.960547  0.960373\n",
              "79           35  0.965308  0.963869  0.964177  0.963869\n",
              "80           40  0.960164  0.959207  0.959395  0.959207\n",
              "81           40  0.959421  0.958042  0.958281  0.958042"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "( np.array(ensemble_data.loc[(ensemble_data[\"CNN%\"] >= 1.0) & (ensemble_data[\"NumOfModels\"]==20),[\"NumOfModels\",\"Pr\",\"Rc\",\"F1\",\"Acc\"]].mean(axis=0)) , \n",
        "  np.array(ensemble_data.loc[(ensemble_data[\"CNN%\"] >= 1.0) & (ensemble_data[\"NumOfModels\"]==40),[\"NumOfModels\",\"Pr\",\"Rc\",\"F1\",\"Acc\"]].mean(axis=0)) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBOWOLe3RoHD",
        "outputId": "05e0072f-db21-43bc-eeb9-5789652472aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([20.        ,  0.95854428,  0.95745921,  0.95755402,  0.95745921]),\n",
              " array([40.        ,  0.95979251,  0.95862471,  0.95883819,  0.95862471]))"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(ensemble_data.loc[(ensemble_data[\"CNN%\"] == 0.0) & (ensemble_data[\"NumOfModels\"]==12),[\"NumOfModels\",\"Pr\",\"Rc\",\"F1\",\"Acc\"]]).mean(axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wi0d0j7sUPrr",
        "outputId": "87a276e1-eb8e-41bd-c7d4-6f3250e004d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([12.        ,  0.92936153,  0.92657343,  0.92470842,  0.92657343])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(ensemble_data.loc[(ensemble_data[\"CNN%\"] == 0.5) & (ensemble_data[\"NumOfModels\"]==12),[\"NumOfModels\",\"Pr\",\"Rc\",\"F1\",\"Acc\"]]).mean(axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myeviXUHmcmY",
        "outputId": "984f2b44-144a-4cc8-c3e5-1b7d04becf5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([12.        ,  0.96461972,  0.96386946,  0.96385723,  0.96386946])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "H8EH2A_qotDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show fully integrated models"
      ],
      "metadata": {
        "id": "WJL3FDzTCMeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals\")"
      ],
      "metadata": {
        "id": "3aJcGIqVCOtB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c5a29bc-6eb9-4afa-c181-ecad91611552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['CNN256_A_1_20211124110828_saved_model_after_fit',\n",
              " 'CNN256_A_2_20211124111459_saved_model_after_fit',\n",
              " 'CNN256_A_3_20211124112133_saved_model_after_fit',\n",
              " 'CNN256_A_4_20211124112755_saved_model_after_fit',\n",
              " 'CNN256_A_5_20211124113440_saved_model_after_fit',\n",
              " 'CNN256_A_6_20211124114114_saved_model_after_fit',\n",
              " 'CNN256_A_7_20211124114753_saved_model_after_fit',\n",
              " 'CNN256_A_8_20211124115418_saved_model_after_fit',\n",
              " 'CNN256_A_9_20211124120100_saved_model_after_fit',\n",
              " 'CNN256_A_10_20211124120744_saved_model_after_fit',\n",
              " 'CNN256_A_11_20211124121436_saved_model_after_fit',\n",
              " 'CNN256_A_12_20211124122043_saved_model_after_fit',\n",
              " 'CNN256_A_13_20211124122718_saved_model_after_fit',\n",
              " 'CNN256_A_14_20211124123410_saved_model_after_fit',\n",
              " 'CNN256_A_15_20211124124025_saved_model_after_fit',\n",
              " 'CNN256_A_16_20211124124717_saved_model_after_fit',\n",
              " 'CNN256_A_17_20211124125352_saved_model_after_fit',\n",
              " 'CNN256_A_18_20211124130025_saved_model_after_fit',\n",
              " 'CNN256_A_19_20211124130651_saved_model_after_fit',\n",
              " 'CNN256_A_20_20211124131334_saved_model_after_fit',\n",
              " 'CNN256_A_21_20211124132004_saved_model_after_fit',\n",
              " 'CNN256_A_22_20211124132639_saved_model_after_fit',\n",
              " 'CNN256_A_23_20211124133316_saved_model_after_fit',\n",
              " 'CNN256_A_24_20211124134008_saved_model_after_fit',\n",
              " 'CNN256_A_25_20211124134640_saved_model_after_fit',\n",
              " 'CNN256_A_26_20211124135322_saved_model_after_fit',\n",
              " 'CNN256_A_27_20211124140019_saved_model_after_fit',\n",
              " 'CNN256_A_28_20211124140641_saved_model_after_fit',\n",
              " 'CNN256_A_29_20211124141316_saved_model_after_fit',\n",
              " 'CNN256_A_30_20211124141929_saved_model_after_fit',\n",
              " 'CNN256_A_31_20211124142615_saved_model_after_fit',\n",
              " 'CNN256_A_32_20211124143302_saved_model_after_fit',\n",
              " 'CNN256_A_33_20211124143941_saved_model_after_fit',\n",
              " 'CNN256_A_34_20211124144626_saved_model_after_fit',\n",
              " 'CNN256_A_35_20211124145317_saved_model_after_fit',\n",
              " 'CNN256_A_36_20211124145950_saved_model_after_fit',\n",
              " 'CNN256_A_37_20211124150632_saved_model_after_fit',\n",
              " 'CNN256_A_38_20211124151259_saved_model_after_fit',\n",
              " 'CNN256_A_39_20211124151925_saved_model_after_fit',\n",
              " 'CNN256_A_40_20211124152552_saved_model_after_fit',\n",
              " 'CNN256_A_41_20211124153222_saved_model_after_fit',\n",
              " 'CNN256_A_42_20211124153847_saved_model_after_fit',\n",
              " 'CNN256_A_43_20211124154518_saved_model_after_fit',\n",
              " 'CNN256_A_44_20211124155140_saved_model_after_fit',\n",
              " 'CNN256_A_45_20211124155829_saved_model_after_fit',\n",
              " 'CNN256_A_46_20211124160503_saved_model_after_fit',\n",
              " 'CNN256_A_47_20211124161120_saved_model_after_fit',\n",
              " 'CNN256_A_48_20211124161756_saved_model_after_fit',\n",
              " 'CNN256_A_49_20211124162434_saved_model_after_fit',\n",
              " 'CNN256_A_50_20211124163127_saved_model_after_fit',\n",
              " 'CNN256_A_51_20211124163815_saved_model_after_fit',\n",
              " 'CNN256_A_52_20211124164436_saved_model_after_fit',\n",
              " 'CNN256_A_53_20211124165059_saved_model_after_fit',\n",
              " 'CNN256_A_54_20211124165804_saved_model_after_fit',\n",
              " 'CNN256_A_55_20211124170430_saved_model_after_fit',\n",
              " 'CNN256_A_56_20211124171118_saved_model_after_fit',\n",
              " 'CNN256_A_57_20211124171753_saved_model_after_fit',\n",
              " 'CNN256_A_58_20211124172421_saved_model_after_fit',\n",
              " 'CNN256_A_59_20211124173056_saved_model_after_fit',\n",
              " 'CNN256_A_60_20211124173748_saved_model_after_fit',\n",
              " 'CNN256_A_61_20211124174423_saved_model_after_fit',\n",
              " 'CNN256_A_62_20211124175107_saved_model_after_fit',\n",
              " 'CNN256_A_63_20211124175727_saved_model_after_fit',\n",
              " 'CNN256_A_64_20211124180355_saved_model_after_fit',\n",
              " 'CNN256_A_65_20211124181033_saved_model_after_fit',\n",
              " 'CNN256_A_66_20211124181717_saved_model_after_fit',\n",
              " 'CNN256_A_67_20211124182357_saved_model_after_fit',\n",
              " 'CNN256_A_68_20211124183028_saved_model_after_fit',\n",
              " 'CNN256_A_69_20211124183704_saved_model_after_fit',\n",
              " 'CNN256_A_70_20211124184353_saved_model_after_fit',\n",
              " 'CNN256_A_71_20211124185022_saved_model_after_fit',\n",
              " 'CNN256_A_72_20211124185651_saved_model_after_fit',\n",
              " 'CNN256_A_73_20211124190333_saved_model_after_fit',\n",
              " 'CNN256_A_74_20211124191010_saved_model_after_fit',\n",
              " 'CNN256_A_75_20211124191705_saved_model_after_fit',\n",
              " 'CNN256_A_76_20211124192348_saved_model_after_fit',\n",
              " 'CNN256_A_77_20211124193011_saved_model_after_fit',\n",
              " 'CNN256_A_78_20211124193648_saved_model_after_fit',\n",
              " 'CNN256_A_79_20211124194316_saved_model_after_fit',\n",
              " 'CNN256_A_80_20211124194946_saved_model_after_fit',\n",
              " 'CNN256_A_81_20211124195552_saved_model_after_fit',\n",
              " 'CNN256_A_82_20211124200226_saved_model_after_fit',\n",
              " 'CNN256_A_83_20211124200859_saved_model_after_fit',\n",
              " 'CNN256_A_84_20211124201537_saved_model_after_fit',\n",
              " 'CNN256_A_85_20211124202231_saved_model_after_fit',\n",
              " 'CNN256_A_86_20211124202928_saved_model_after_fit',\n",
              " 'CNN256_A_87_20211124203619_saved_model_after_fit',\n",
              " 'CNN256_A_88_20211124204306_saved_model_after_fit',\n",
              " 'CNN256_A_89_20211124204936_saved_model_after_fit',\n",
              " 'CNN256_A_90_20211124205551_saved_model_after_fit',\n",
              " 'CNN256_A_91_20211124210233_saved_model_after_fit',\n",
              " 'CNN256_A_92_20211124210923_saved_model_after_fit',\n",
              " 'CNN256_A_93_20211124211555_saved_model_after_fit',\n",
              " 'CNN256_A_94_20211124212215_saved_model_after_fit',\n",
              " 'CNN256_A_95_20211124212859_saved_model_after_fit',\n",
              " 'CNN256_A_96_20211124213532_saved_model_after_fit',\n",
              " 'CNN256_A_97_20211124214206_saved_model_after_fit',\n",
              " 'CNN256_A_98_20211124214843_saved_model_after_fit',\n",
              " 'CNN256_A_99_20211124215531_saved_model_after_fit',\n",
              " 'CNN256_A_100_20211124220210_saved_model_after_fit',\n",
              " 'RNN_A_1_20211125061523_saved_model_after_fit',\n",
              " 'RNN_A_2_20211130131128_saved_model_after_fit',\n",
              " 'RNN_A_3_20211201005100_saved_model_after_fit',\n",
              " 'RNN_A_101_20211202161946_saved_model_after_fit',\n",
              " 'RNN_A_102_20211203065818_saved_model_after_fit',\n",
              " 'RNN_A_101B_20211202161946_saved_model_after_fit',\n",
              " 'RNN_A_201_20211204013934_saved_model_after_fit',\n",
              " 'RNN_A_301_20211204152506_saved_model_after_fit',\n",
              " 'RNN_A_401_20211205025618_saved_model_after_fit',\n",
              " 'RNN_A_302_20211205171608_saved_model_after_fit',\n",
              " 'RNN_A_302B_20211205171608_saved_model_after_fit',\n",
              " 'RNN_A_402_20211205185555_saved_model_after_fit',\n",
              " 'ParallelCNN_A_20211209020220_saved_model_after_fit',\n",
              " 'ParallelCNN_A2_20211209133850_saved_model_after_fit',\n",
              " 'ParallelCNNRNN_A2_20211210221624_saved_model_after_fit']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn20_par = load_model(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/ParallelCNN_A_20211209020220_saved_model_after_fit\")\n",
        "\n"
      ],
      "metadata": {
        "id": "f69Wf3h-CVv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn2_par = load_model(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/ParallelCNN_A2_20211209133850_saved_model_after_fit\")"
      ],
      "metadata": {
        "id": "2W28lvYICf-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn1rnn1_par = load_model(f\"/content/drive/MyDrive/data_papers/{paper_name}/model_finals/ParallelCNNRNN_A2_20211210221624_saved_model_after_fit\")"
      ],
      "metadata": {
        "id": "M2szYsA6Cm8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn20_par_y_predict_here = np.array(cnn20_par.predict(X_test_1000e), dtype='float64')\n",
        "cnn20_par_y_predict_here = np.apply_along_axis(np.argmax, 1, cnn20_par_y_predict_here)\n",
        "pr_rc_f1_acc_from_supplied(cnn20_par_y_predict_here, np.apply_along_axis(np.argmax, 1, Y_test_1000e))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deJ1xAJOCtrQ",
        "outputId": "cd2d21c5-3a1e-4107-e5fd-eceb3f59035c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9318062855471186,\n",
              " 0.9184149184149184,\n",
              " 0.9195749976697266,\n",
              " 0.9184149184149184)"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn2_par_y_predict_here = np.array(cnn2_par.predict(X_test_1000e), dtype='float64')\n",
        "cnn2_par_y_predict_here = np.apply_along_axis(np.argmax, 1, cnn2_par_y_predict_here)\n",
        "pr_rc_f1_acc_from_supplied(cnn2_par_y_predict_here, np.apply_along_axis(np.argmax, 1, Y_test_1000e))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpQnCeauJ8Rl",
        "outputId": "a76d59f9-8116-40aa-88fd-923a1ac5729b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9346676642409038,\n",
              " 0.9289044289044289,\n",
              " 0.9267679288900833,\n",
              " 0.9289044289044289)"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn1rnn1_par_y_predict_here = np.array(cnn1rnn1_par.predict(X_test_1000e), dtype='float64')\n",
        "cnn1rnn1_par_y_predict_here = np.apply_along_axis(np.argmax, 1, cnn1rnn1_par_y_predict_here)\n",
        "pr_rc_f1_acc_from_supplied(cnn1rnn1_par_y_predict_here, np.apply_along_axis(np.argmax, 1, Y_test_1000e))"
      ],
      "metadata": {
        "id": "9blknBwJKDlE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "986ec5a7-6663-40a8-be15-bc1b6ae08be6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9537472379437839, 0.951048951048951, 0.9517260643724785, 0.951048951048951)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IEw1CQqRaOOS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}